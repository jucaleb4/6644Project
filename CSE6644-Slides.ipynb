{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "1. **Given a linear or nonlinear system, choose an appropriate numerical solution method based on the properties of the system**: \n",
    "\n",
    "2. **Evaluate a method for its convergence and computational cost, including parallel computing aspects**\n",
    "\n",
    "3. **Diagnose convergence problems of iterative solution methods**\n",
    "\n",
    "4. **Select or design a method or approach for preconditioning the solution of specific problems**\n",
    "\n",
    "5. **Use [scipy], [PETSc], [petsc] or other numerical software for solving systems of equations**\n",
    "\n",
    "\n",
    "## Model Problem\n",
    "\n",
    "We explore the theoretical and empirical performance of the proposed Jacobian-free Newtons method using preconditioned Krylov subspace methods for solving nonlinear optimization problem based on [this paper](https://epubs.siam.org/doi/abs/10.1137/0905039?casa_token=5o2ebQDJ-P0AAAAA:FzkM1jsrCALfpTZ9jA6gU4EOiWlZ1RRWVV8UbhrZ_5Kylnw4zHiiY-Bh1p8Iw_lhlT2j0Zvjng) and compare it with other exact and approximate Jacobian-based methods. Our first model problem is the 1-D nonlinear PDE,\n",
    "\n",
    "$$-u_{xx} + 2b(e^u)_x + ce^u = R(x) \\text{ on the domain } 0 < x < 1$$\n",
    "\n",
    "with homogeneous Dirichlet boundary conditions, or $u_0=u_{n+1}=0$. To numerically solve this, we discretize it on a uniform gird over $n$ itervals using centered finite differencing. We write its formulation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_PDE(u):\n",
    "    \"\"\" Centered finite difference of nonlinear PDE\n",
    "            -u_xx + 2b(e^u)_x + ce^u = R(x) for x \\in [0,1]\n",
    "        with homogenous Dirichlet boundary conditions.\n",
    "    \"\"\"\n",
    "    n = len(u)\n",
    "    h = 1 / (n + 1)\n",
    "    # TODO: allow @b,@c to be parameters\n",
    "    b = c = 1\n",
    "\n",
    "    expu = np.exp(u)\n",
    "\n",
    "    r1 = (1/h**2) * (sp.diags([1, -2, 1], [-1, 0, 1], shape=(n, n)).dot(u))\n",
    "    r2 = (1/(2 * h)) * (sp.diags([-1, 0, 1], [-1, 0, 1], shape=(n, n)).dot(expu))\n",
    "    r3 = expu\n",
    "\n",
    "    r = -r1 + 2*b*r2 + c*r3\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rafael Includes 2nd Model Problem If He Gets the Chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvergenceTracker():\n",
    "    \"\"\" Stores information such as iteration counts and residuals for\n",
    "    analyzing convergence of iterative method.\"\"\"\n",
    "    def __init__(self, initial_res=None,label='Convergence History'):\n",
    "        self._iter_count = 0\n",
    "        self._residuals = [initial_res]\n",
    "        self._label = label\n",
    "\n",
    "    def __call__(self, residual=None):\n",
    "        self._iter_count += 1\n",
    "        self._residuals.append(residual)\n",
    "\n",
    "    def niters(self):\n",
    "        return self._iter_count\n",
    "    \n",
    "    def plot(self, ax=plt.gca):\n",
    "        ax.semilogy(range(self._iter_count+1),self._residuals,label=self._label)x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Given a nonlinear objective function $F$ and its Jacobian $F_J$, where we want to find $x^\\star$ such that $F(x^\\star) = 0$, we can find the solution using Newton's method. The template code is as follows:\n",
    "\n",
    "1. Start with an initial guess $x_0$\n",
    "2. Repeat until convergence\n",
    "    1. Solve $J_F(x_i)s = -F(x_i)$\n",
    "    2. Set $x_{i+1} \\leftarrow x_i + s$\n",
    "    \n",
    "For our problem at hand, we want to find a solution $u$ that satisfies the PDEs described above. Therefore, we can model our problem as\n",
    "\n",
    "$$F(u) = PDE(u) - R(x)$$\n",
    "\n",
    "where $PDE(u)$ is the finite-differnce PDE evaluated with solution $u$ (as formulated above) and $R(x)$ is the true solution of the PDE. Now, we need the Jacobian of system from the finite differencing of the PDE. This can be derivied either analytically or by using finite differencing once more. We include both codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_jacobian(F, u):\n",
    "    \"\"\" Exact Jacobian of finite difference model PDE from above \"\"\"\n",
    "    n = len(u)\n",
    "    h = 1 / (n + 1)\n",
    "    # TODO: allow @b,@c to be parameters\n",
    "    b = c = 1\n",
    "    expu = np.exp(u)\n",
    "\n",
    "    subdiag = -(1 / h**2) - 2 * b / (2 * h) * expu[:n - 1]\n",
    "    maindiag = 2 / (h**2) + c * expu\n",
    "    supdiag = -(1 / h**2) + 2 * b / (2 * h) * expu[1:]\n",
    "\n",
    "    J = sp.diags(subdiag, -1) + sp.diags(maindiag, 0) + sp.diags(supdiag, 1)\n",
    "\n",
    "    return J\n",
    "\n",
    "def fd_jacobian(F, u):\n",
    "    \"\"\" Finite Difference Approximation of Jacobian for any function F.\n",
    "\n",
    "    - F : Function eval. Returns 1d np.ndarray\n",
    "    - x : Current solution\n",
    "\n",
    "    Returns: @J csr_matrix\n",
    "    \"\"\"\n",
    "    n = len(u)\n",
    "    J = np.zeros((n, n))\n",
    "    slen = 1e-4\n",
    "    Fu = F(u)\n",
    "\n",
    "    for i in range(n):\n",
    "        e_i = np.append(np.zeros(i), np.append(1, np.zeros(n - i - 1)))\n",
    "        # TODO: Drop terms with small values to induce sparsity?\n",
    "        J[:, i] = (F(u + slen * e_i) - Fu) / slen\n",
    "\n",
    "    return sp.csr_matrix(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we write the code for Newton's method with a matrix of the Jacobian, we observe that in each iteration of Newton's method we need to solve the linear system $J_F(x_i)s = -F(x_i)$. We consider three methods for solving this:\n",
    "\n",
    "- Sparse direct solver\n",
    "- GMRES\n",
    "- BiCG? CGS?\n",
    "\n",
    "We recall the convergence of GMRES for solving $Ax=b$ depends on the residual bound. Assuming the matrix $A$ is diagonalizable via $A=X^{-1}\\Lambda X$ where $\\Lambda=\\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_n)$, then we at the $m$ iteration that\n",
    "\n",
    "$$ \\frac{\\|r_m \\|_2}{\\|r_0\\|_2} \\leq \\kappa_2(X) \\min_{p \\in \\mathbb{P}_m, p(0)=1} \\max_{i=1,\\ldots,n} |p(\\lambda_i)|$$\n",
    "\n",
    "In other words, the non-normality of the Jacobian matrix $J$ can impact the convergence when using GMRES. Below, we examine $\\kappa(X)$ for the exact and finite-difference Jacobian matrix with different solutions for $u$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # fix randomness \n",
    "\n",
    "# which solution @u we use\n",
    "for i in range(2):\n",
    "    # exact or FD jacobian\n",
    "    for j in range(2):\n",
    "        # size of problem\n",
    "        for k in range(1,4):\n",
    "            \n",
    "            n = 10**k\n",
    "            \n",
    "            if(i==0): u = np.ones(n)\n",
    "            else: u = 10*np.random.random(n)\n",
    "                \n",
    "            if(j==0): J = exact_jacobian(nonlinear_PDE, u)\n",
    "            else: J = fd_jacobian(nonlinear_PDE, u)\n",
    "                \n",
    "            _,X = la.eig(J.todense())\n",
    "            condX = la.cond(X)\n",
    "            \n",
    "            print(\"Cond(X) for {} Jacobian of size n={:<4} with {} solution => {:.4e}\".format(\\\n",
    "                    \"exact\" if j==0 else \"FD\",\n",
    "                    n,\n",
    "                    \"uniform\" if i==0 else \"random\",\n",
    "                    condX))\n",
    "            \n",
    "        print(\"===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see that the choice of exact or finite difference Jacobian does not make much of a difference theoretically. However, the true solution $u^\\star$ and any approximate solution $u$ encountered during the Newton's step can severely hamper the convergence of GMRES potentially. \n",
    "\n",
    "With this in mind, we move onto Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons(F, J, x0, tol=1e-6, omega=1, solver='Direct'):\n",
    "    \"\"\" Newton's Method with exact or FD Jacobian\n",
    "\n",
    "    - F : Function eval. Returns 1d np.ndarray\n",
    "    - J : Jacobian eval. Returns 2d np.ndarray\n",
    "    - x0 : Initial guess. 1D np.ndarray\n",
    "\n",
    "    Returns:\n",
    "    - @x solution so that F(x)=0\n",
    "    - @con_tracker ConvergenceTracker object\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    con_tracker = ConvergenceTracker(la.norm(F(x)))\n",
    "    err = 1\n",
    "    while (la.norm(F(x)) > tol and err > tol):\n",
    "        A = J(F, x)\n",
    "        xOld = x\n",
    "        # use solver with/without preconditioner\n",
    "        # TODO: When will this be singular?\n",
    "        # TODO: Allow other methods like GMRES and maybe CGS\n",
    "        if solver == 'GMRES':\n",
    "            M = lin_ssor_precon(A,omega)\n",
    "            s, _ = spla.gmres(A, -F(x), M=M, callback=con_tracker,restart=100)\n",
    "            x = x+s\n",
    "        else:\n",
    "            s = spla.spsolve(A, -F(x))\n",
    "            x = x + s\n",
    "            con_tracker(la.norm(F(x)))\n",
    "        err = la.norm(x-xOld)/la.norm(x)\n",
    "\n",
    "    return x, con_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all we need to use Newton's method with direct sparse solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    ns = [10, 50, 100, 500, 1000]\n",
    "ns = [10, 20, 30, 40]\n",
    "seed_num = np.random.randint(0, 1000)\n",
    "print(\"== Seed {} ==\\n\".format(seed_num))\n",
    "func_evals_exact = []\n",
    "func_evals_fd = []\n",
    "con_trackers_exact = []\n",
    "con_trackers_fd = []\n",
    "for n in ns:\n",
    "    # utrue = np.sin(np.arange(n))\n",
    "    utrue = np.ones(n)\n",
    "    R = nonlinear_PDE(utrue)\n",
    "    n_func_evals = 0\n",
    "\n",
    "    # Objective function\n",
    "    def F(v):\n",
    "        global n_func_evals\n",
    "        n_func_evals += 1\n",
    "        return nonlinear_PDE(v) - R\n",
    "\n",
    "    # Starting guess\n",
    "\n",
    "    np.random.seed(seed_num)\n",
    "    u0 = np.random.normal(size=n)\n",
    "\n",
    "    # Exact Jacobian direct solve\n",
    "    u, con_tracker_ex = newtons(F, exact_jacobian, u0, tol=1e-6)\n",
    "    print(\"\\n>>Exact Jacobian: Direct solve\")\n",
    "    print(\"Converged in {} iterations\".format(con_tracker_ex.niters()))\n",
    "    print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "    print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "    func_evals_exact.append(n_func_evals)\n",
    "    con_tracker_ex._label=\"Exact Jacobian n={}\".format(n)\n",
    "    con_trackers_exact.append(con_tracker_ex)\n",
    "\n",
    "    n_func_evals = 0\n",
    "    # Finite difference Jacobian direct solve\n",
    "    u, con_tracker_fd = newtons(F, fd_jacobian, u0, tol=1e-6)\n",
    "    print(\"\\n>> FD Jacobian: Direct solve\")\n",
    "    print(\"Converged in {} iterations\".format(con_tracker_fd.niters()))\n",
    "    print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "    print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "    func_evals_fd.append(n_func_evals)\n",
    "    con_tracker_fd._label=\"FD Jacobian n={}\".format(n)\n",
    "    con_trackers_fd.append(con_tracker_fd)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for tracker in con_trackers_exact:\n",
    "    tracker.plot(ax)\n",
    "for tracker in con_trackers_fd:\n",
    "    tracker.plot(ax)\n",
    "_ = ax.legend(loc='upper right', fontsize='xx-small')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Residual Norm')\n",
    "plt.title('Direct Solve Convergence')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "plt.xlabel('Problem Size (n)')\n",
    "plt.ylabel('Function Evaluations')\n",
    "ax2.loglog(ns,func_evals_exact,label='Exact Jacobian')\n",
    "ax2.loglog(ns,func_evals_fd,label='FD Jacobian')\n",
    "_ = ax2.legend(loc='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are only tracking the residual on each outer iteration. So we do not actually measure the work done in `spsolve` but we know that an LU factorization and backsolving takes INSERT COMPLEXITY HERE flops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the process using un-preconditioned GMRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def lin_ssor_precon(A,omega):\n",
    "        return None\n",
    "    \n",
    "    ns = [10, 50, 100, 500, 1000]\n",
    "    func_evals_exact = []\n",
    "    func_evals_fd = []\n",
    "    con_trackers_exact = []\n",
    "    con_trackers_fd = []\n",
    "    for n in ns:\n",
    "        # utrue = np.sin(np.arange(n))\n",
    "        utrue = np.ones(n)\n",
    "        R = nonlinear_PDE(utrue)\n",
    "        n_func_evals = 0\n",
    "\n",
    "        # Objective function\n",
    "        def F(v):\n",
    "            global n_func_evals\n",
    "            n_func_evals += 1\n",
    "            return nonlinear_PDE(v) - R\n",
    "\n",
    "        # Starting guess\n",
    "\n",
    "        np.random.seed(seed_num)\n",
    "        u0 = np.random.normal(size=n)\n",
    "\n",
    "        # Exact Jacobian direct solve\n",
    "        u, con_tracker_ex = newtons(F, exact_jacobian, u0, tol=1e-6,solver='GMRES')\n",
    "        print(\"\\n>>Exact Jacobian: GMRES\")\n",
    "        print(\"Converged in {} iterations\".format(con_tracker_ex.niters()))\n",
    "        print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "        print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "        func_evals_exact.append(n_func_evals)\n",
    "        con_tracker_ex._label=\"Exact Jacobian n={}\".format(n)\n",
    "        con_trackers_exact.append(con_tracker_ex)\n",
    "\n",
    "        n_func_evals = 0\n",
    "        # Finite difference Jacobian direct solve\n",
    "        u, con_tracker_fd = newtons(F, fd_jacobian, u0, tol=1e-6,solver='GMRES')\n",
    "        print(\"\\n>> FD Jacobian: GMRES\")\n",
    "        print(\"Converged in {} iterations\".format(con_tracker_fd.niters()))\n",
    "        print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "        print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "        func_evals_fd.append(n_func_evals)\n",
    "        con_tracker_fd._label=\"FD Jacobian n={}\".format(n)\n",
    "        con_trackers_fd.append(con_tracker_fd)\n",
    "\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for tracker in con_trackers_exact:\n",
    "        tracker.plot(ax)\n",
    "    for tracker in con_trackers_fd:\n",
    "        tracker.plot(ax)\n",
    "    _ = ax.legend(loc='upper right', fontsize='xx-small')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Residual Norm')\n",
    "    plt.title('GMRES Convergence')\n",
    "    \n",
    "    fig2 = plt.figure()\n",
    "    ax2 = fig2.add_subplot(111)\n",
    "    plt.xlabel('Problem Size (n)')\n",
    "    plt.ylabel('Function Evaluations')\n",
    "    ax2.loglog(ns,func_evals_exact,label='Exact Jacobian')\n",
    "    ax2.loglog(ns,func_evals_fd,label='FD Jacobian')\n",
    "    _ = ax2.legend(loc='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without a preconditioner, GMRES takes many iterations. Blah blah blah..... Let's add a preconditioner now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_ssor_precon(A, omega):\n",
    "    \"\"\" Construct an linear ssor preconditioner for the cases when we have a jacobian\n",
    "    matrix\"\"\"\n",
    "\n",
    "    #get the D, L and U parts of J\n",
    "    n = A.shape[0]\n",
    "    L = sp.tril(A, k=-1)\n",
    "    D = sp.diags(A.diagonal())\n",
    "    D_inv = sp.diags(1 / A.diagonal())\n",
    "    U = sp.triu(A, k=1)\n",
    "\n",
    "    M_1 = D_inv @ (D - omega * U)\n",
    "    M_2 = omega * (2 - omega) * (D - omega * L)\n",
    "\n",
    "    def mv(v):\n",
    "        w = M_2 @ M_1 @ v\n",
    "\n",
    "        return w\n",
    "\n",
    "    return spla.LinearOperator((n, n), matvec=mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #   ns = [10, 50, 100, 500,1000]\n",
    "    func_evals_exact = []\n",
    "    func_evals_fd = []\n",
    "    con_trackers_exact = []\n",
    "    con_trackers_fd = []\n",
    "    for n in ns:\n",
    "        # utrue = np.sin(np.arange(n))\n",
    "        utrue = np.ones(n)\n",
    "        R = nonlinear_PDE(utrue)\n",
    "        n_func_evals = 0\n",
    "\n",
    "        # Objective function\n",
    "        def F(v):\n",
    "            global n_func_evals\n",
    "            n_func_evals += 1\n",
    "            return nonlinear_PDE(v) - R\n",
    "\n",
    "        # Starting guess\n",
    "\n",
    "        np.random.seed(seed_num)\n",
    "        u0 = np.random.normal(size=n)\n",
    "\n",
    "        # Exact Jacobian direct solve\n",
    "        u, con_tracker_ex = newtons(F, exact_jacobian, u0, tol=1e-6,solver='GMRES')\n",
    "        print(\"\\n>>Exact Jacobian: GMRES\")\n",
    "        print(\"Converged in {} iterations\".format(con_tracker_ex.niters()))\n",
    "        print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "        print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "        func_evals_exact.append(n_func_evals)\n",
    "        con_tracker_ex._label=\"Exact Jacobian n={}\".format(n)\n",
    "        con_trackers_exact.append(con_tracker_ex)\n",
    "\n",
    "        n_func_evals = 0\n",
    "        # Finite difference Jacobian direct solve\n",
    "        u, con_tracker_fd = newtons(F, fd_jacobian, u0, tol=1e-6,solver='GMRES')\n",
    "        print(\"\\n>> FD Jacobian: GMRES\")\n",
    "        print(\"Converged in {} iterations\".format(con_tracker_fd.niters()))\n",
    "        print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "        print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "        func_evals_fd.append(n_func_evals)\n",
    "        con_tracker_fd._label=\"FD Jacobian n={}\".format(n)\n",
    "        con_trackers_fd.append(con_tracker_fd)\n",
    "\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for tracker in con_trackers_exact:\n",
    "        tracker.plot(ax)\n",
    "    for tracker in con_trackers_fd:\n",
    "        tracker.plot(ax)\n",
    "    _ = ax.legend(loc='upper right', fontsize='xx-small')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Residual Norm')\n",
    "    plt.title('GMRES Convergence')\n",
    "    \n",
    "    fig2 = plt.figure()\n",
    "    ax2 = fig2.add_subplot(111)\n",
    "    plt.xlabel('Problem Size (n)')\n",
    "    plt.ylabel('Function Evaluations')\n",
    "    ax2.loglog(ns,func_evals_exact,label='Exact Jacobian')\n",
    "    ax2.loglog(ns,func_evals_fd,label='FD Jacobian')\n",
    "    _ = ax2.legend(loc='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also very the $\\omega$ parameter for the preconditioner to see its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "omegas = np.linspace(1e-8,2-1e-8,20)\n",
    "func_evals_exact = []\n",
    "func_evals_fd = []\n",
    "con_trackers_exact = []\n",
    "con_trackers_fd = []\n",
    "\n",
    "for omega in omegas:\n",
    "    utrue = np.ones(n)\n",
    "    R = nonlinear_PDE(utrue)\n",
    "    n_func_evals = 0\n",
    "\n",
    "    # Objective function\n",
    "    def F(v):\n",
    "        global n_func_evals\n",
    "        n_func_evals += 1\n",
    "        return nonlinear_PDE(v) - R\n",
    "\n",
    "    # Starting guess\n",
    "\n",
    "    np.random.seed(seed_num)\n",
    "    u0 = np.random.normal(size=n)\n",
    "\n",
    "    # Exact Jacobian direct solve\n",
    "    u, con_tracker_ex = newtons(F, exact_jacobian, u0, tol=1e-6,omega=omega,solver='GMRES')\n",
    "    print(\"\\n>>Exact Jacobian: GMRES\")\n",
    "    print(\"Converged in {} iterations\".format(con_tracker_ex.niters()))\n",
    "    print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "    print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "    func_evals_exact.append(n_func_evals)\n",
    "    con_tracker_ex._label=\"Exact Jacobian n={}\".format(n)\n",
    "    con_trackers_exact.append(con_tracker_ex)\n",
    "\n",
    "    n_func_evals = 0\n",
    "    # Finite difference Jacobian direct solve\n",
    "    u, con_tracker_fd = newtons(F, fd_jacobian, u0, tol=1e-6,omega=omega,solver='GMRES')\n",
    "    print(\"\\n>> FD Jacobian: GMRES\")\n",
    "    print(\"Converged in {} iterations\".format(con_tracker_fd.niters()))\n",
    "    print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "    print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "    func_evals_fd.append(n_func_evals)\n",
    "    con_tracker_fd._label=\"FD Jacobian n={}\".format(n)\n",
    "    con_trackers_fd.append(con_tracker_fd)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fd_iters = [c.niters() for c in con_trackers_fd]\n",
    "ex_iters = [c.niters() for c in con_trackers_exact]\n",
    "ax.plot(omegas,ex_iters, label='Exact Jacobian')\n",
    "ax.plot(omegas,fd_iters,label='FD Jacobian')\n",
    "plt.xlabel('$\\omega$')\n",
    "plt.ylabel('Number of GMRES iterations')\n",
    "_ = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that a value of $\\omega=1$ is not necessarily optimal, but does a pretty decent job of preconditioning.\n",
    "\n",
    "\n",
    "Now, all of the work up to this point has relied on explicitly forming the Jacobian matrices used in the Newton step. This has disadvantages of BLAH BLAH and so we would like to get the same or similar results without building these matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_derfree(F, x0, omega=1, tol=1e-6):\n",
    "    \"\"\" Newton's Method with Directional Derivative\n",
    "\n",
    "    - F : Function eval. Returns 1d np.ndarray\n",
    "    - x0 : Initial guess. 1D np.ndarray\n",
    "\n",
    "    Returns:\n",
    "    - @x solution so that F(x)=0\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    n = len(x)\n",
    "    slen = 1e-4\n",
    "    con_tracker = ConvergenceTracker(la.norm(F(x)))\n",
    "    err = 1\n",
    "\n",
    "    while (la.norm(F(x)) > tol and err > tol):\n",
    "        # use solver with/without preconditioner\n",
    "        def mv(v):\n",
    "            return (F(x + slen * v) - F(x)) / slen\n",
    "        J = spla.LinearOperator((n, n), matvec=mv)\n",
    "        \n",
    "        xOld = x\n",
    "        # TODO: Use Krylov method here\n",
    "        M = ssor_precon(F, omega, x)\n",
    "        if M == None:\n",
    "            M = spla.aslinearoperator(sp.eye(n))\n",
    "        s, _ = spla.gmres(M@J, M@(-F(x)), callback=con_tracker,restart=100)\n",
    "        x = x + s\n",
    "        err = la.norm(x-xOld)/la.norm(x)\n",
    "\n",
    "    return x, con_tracker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we play the same games as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssor_precon(F,omega,x):\n",
    "    return None\n",
    "#ns= [10, 50, 100, 500,1000]\n",
    "func_evals_derfree = []\n",
    "con_trackers_derfree = []\n",
    "for n in ns:\n",
    "    # utrue = np.sin(np.arange(n))\n",
    "    utrue = np.ones(n)\n",
    "    R = nonlinear_PDE(utrue)\n",
    "    n_func_evals = 0\n",
    "\n",
    "    # Objective function\n",
    "    def F(v):\n",
    "        global n_func_evals\n",
    "        n_func_evals += 1\n",
    "        return nonlinear_PDE(v) - R\n",
    "\n",
    "    # Starting guess\n",
    "\n",
    "    np.random.seed(seed_num)\n",
    "    u0 = np.random.normal(size=n)\n",
    "\n",
    "    # Exact Jacobian direct solve\n",
    "    u, con_tracker_derfree = newtons_derfree(F, u0, omega=1, tol=1e-6)\n",
    "    print(\"\\n>>Matrix Free Jacobian: GMRES\")\n",
    "    print(\"Converged in {} iterations\".format(con_tracker_derfree.niters()))\n",
    "    print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "    print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "    func_evals_derfree.append(n_func_evals)\n",
    "    con_tracker_derfree._label=\"Matrix Free Jacobian n={}\".format(n)\n",
    "    con_trackers_derfree.append(con_tracker_derfree)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for tracker in con_trackers_derfree:\n",
    "    tracker.plot(ax)\n",
    "_ = ax.legend(loc='upper right', fontsize='xx-small')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Residual Norm')\n",
    "plt.title('GMRES Convergence')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "plt.xlabel('Problem Size (n)')\n",
    "plt.ylabel('Function Evaluations')\n",
    "ax2.loglog(ns,func_evals_derfree,label='Matrix Free Jacobian')\n",
    "_ = ax2.legend(loc='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the matrix free method does not out perform the other GMRES solvers without a preconditioner. Since we no longer have a matrix to use to build the preconditioner we have to be slightly more clever. BLAH BLAH BLAH here is our matrix free preconditioner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilise(a, small=1e-8):\n",
    "    return a if abs(a) > small else small\n",
    "\n",
    "def ssor_precon(F, omega, x,iter_out_limit=1,iter_in_limit=1):\n",
    "    \"\"\" Construct an ssor preconditioner for the cases when we don't have a jacobian\n",
    "    matrix\"\"\"\n",
    "\n",
    "    tol = 1e-10\n",
    "    eps = 1e-4\n",
    "    n = len(x)\n",
    "    \n",
    "\n",
    "\n",
    "    def mv(v):\n",
    "        err = 1.\n",
    "        n_out_iter = 0\n",
    "        w_old = np.zeros(n)\n",
    "        w = np.zeros(n)\n",
    "        def Fw(w):\n",
    "            return (F(x+eps*w) - F(x))/eps - v\n",
    "\n",
    "        while (err > tol and n_out_iter < iter_out_limit):\n",
    "            w_old = np.copy(w)\n",
    "\n",
    "            for ii in range(2 * n):\n",
    "                i = ii if ii < n else 2 * n - ii - 1\n",
    "\n",
    "                w_i_old = w[i]\n",
    "                inerr = 1.\n",
    "                n_in_iter = 0\n",
    "\n",
    "\n",
    "                eps_e_i = np.append(np.zeros(i),\n",
    "                                    np.append(eps, np.zeros(n - i - 1)))\n",
    "\n",
    "                # Try Newton's method for root finding\n",
    "\n",
    "                while (inerr > tol and n_in_iter < iter_in_limit):\n",
    "                    Fwi = Fw(w)[i]\n",
    "                    Fwi_fwd = Fw(w+eps_e_i)[i]\n",
    "                    Fwi_bwd = Fw(w-eps_e_i)[i]\n",
    "                    dFwi = (Fwi_fwd - Fwi_bwd) / (2 * eps)\n",
    "\n",
    "                    w[i] = w[i] - Fwi / dFwi\n",
    "                    inerr = abs(Fwi/dFwi)\n",
    "                    n_in_iter += 1\n",
    "\n",
    "                # only update ith coordinate\n",
    "                w[i] = (1 - omega) * w_i_old + omega * w[i]\n",
    "\n",
    "            err = la.norm(w - w_old) / stabilise(la.norm(w))\n",
    "            n_out_iter += 1\n",
    "\n",
    "        return w\n",
    "\n",
    "    return spla.LinearOperator((n, n), matvec=mv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns= [10, 50, 100, 500,1000]\n",
    "func_evals_derfree = []\n",
    "con_trackers_derfree = []\n",
    "for n in ns:\n",
    "    # utrue = np.sin(np.arange(n))\n",
    "    utrue = np.ones(n)\n",
    "    R = nonlinear_PDE(utrue)\n",
    "    n_func_evals = 0\n",
    "\n",
    "    # Objective function\n",
    "    def F(v):\n",
    "        global n_func_evals\n",
    "        n_func_evals += 1\n",
    "        return nonlinear_PDE(v) - R\n",
    "\n",
    "    # Starting guess\n",
    "\n",
    "    np.random.seed(seed_num)\n",
    "    u0 = np.random.normal(size=n)\n",
    "\n",
    "    # Exact Jacobian direct solve\n",
    "    u, con_tracker_derfree = newtons_derfree(F, u0, omega=1, tol=1e-6)\n",
    "    print(\"\\n>>Matrix Free Jacobian: GMRES\")\n",
    "    print(\"Converged in {} iterations\".format(con_tracker_derfree.niters()))\n",
    "    print(\"Solution Error ={:.2e}\".format(la.norm(u - utrue) / la.norm(utrue)))\n",
    "    print(\"Used {} function evaluations\".format(n_func_evals))\n",
    "    func_evals_derfree.append(n_func_evals)\n",
    "    con_tracker_derfree._label=\"Exact Jacobian n={}\".format(n)\n",
    "    con_trackers_derfree.append(con_tracker_derfree)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for tracker in con_trackers_derfree:\n",
    "    tracker.plot(ax)\n",
    "_ = ax.legend(loc='upper right', fontsize='xx-small')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Residual Norm')\n",
    "plt.title('GMRES Convergence')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "plt.xlabel('Problem Size (n)')\n",
    "plt.ylabel('Function Evaluations')\n",
    "ax2.loglog(ns,func_evals_derfree,label='Matrix Free Jacobian')\n",
    "_ = ax2.legend(loc='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
