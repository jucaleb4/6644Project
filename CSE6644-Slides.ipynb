{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "1. **Given a linear or nonlinear system, choose an appropriate numerical solution method based on the properties of the system**: \n",
    "\n",
    "2. **Evaluate a method for its convergence and computational cost, including parallel computing aspects**\n",
    "\n",
    "3. **Diagnose convergence problems of iterative solution methods**\n",
    "\n",
    "4. **Select or design a method or approach for preconditioning the solution of specific problems**\n",
    "\n",
    "5. **Use [scipy], [PETSc], [petsc] or other numerical software for solving systems of equations**\n",
    "\n",
    "\n",
    "## Model Problem\n",
    "\n",
    "We explore the theoretical and empirical performance of the proposed Jacobian-free Newtons method using preconditioned Krylov subspace methods for solving nonlinear optimization problem based on [this paper](https://epubs.siam.org/doi/abs/10.1137/0905039?casa_token=5o2ebQDJ-P0AAAAA:FzkM1jsrCALfpTZ9jA6gU4EOiWlZ1RRWVV8UbhrZ_5Kylnw4zHiiY-Bh1p8Iw_lhlT2j0Zvjng) and compare it with other exact and approximate Jacobian-based methods. Our first model problem is the 1-D nonlinear PDE,\n",
    "\n",
    "$$-u_{xx} + 2b(e^u)_x + ce^u = R(x) \\text{ on the domain } 0 < x < 1$$\n",
    "\n",
    "with homogeneous Dirichlet boundary conditions, or $u_0=u_{n+1}=0$. To numerically solve this, we discretize it on a uniform gird over $n$ itervals using centered finite differencing. We write its formulation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_PDE(u):\n",
    "    \"\"\" Centered finite difference of nonlinear PDE\n",
    "            -u_xx + 2b(e^u)_x + ce^u = R(x) for x \\in [0,1]\n",
    "        with homogenous Dirichlet boundary conditions.\n",
    "    \"\"\"\n",
    "    n = len(u)\n",
    "    h = 1 / (n + 1)\n",
    "    # TODO: allow @b,@c to be parameters\n",
    "    b = c = 1\n",
    "\n",
    "    expu = np.exp(u)\n",
    "\n",
    "    r1 = (1/h**2) * (sp.diags([1, -2, 1], [-1, 0, 1], shape=(n, n)).dot(u))\n",
    "    r2 = (1/(2 * h)) * (sp.diags([-1, 0, 1], [-1, 0, 1], shape=(n, n)).dot(expu))\n",
    "    r3 = expu\n",
    "\n",
    "    r = -r1 + 2*b*r2 + c*r3\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rafael Includes 2nd Model Problem If He Gets the Chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Given a nonlinear objective function $F$ and its Jacobian $F_J$, where we want to find $x^\\star$ such that $F(x^\\star) = 0$, we can find the solution using Newton's method. The template code is as follows:\n",
    "\n",
    "1. Start with an initial guess $x_0$\n",
    "2. Repeat until convergence\n",
    "    1. Solve $J_F(x_i)s = -F(x_i)$\n",
    "    2. Set $x_{i+1} \\leftarrow x_i + s$\n",
    "    \n",
    "For our problem at hand, we want to find a solution $u$ that satisfies the PDEs described above. Therefore, we can model our problem as\n",
    "\n",
    "$$F(u) = PDE(u) - R(x)$$\n",
    "\n",
    "where $PDE(u)$ is the finite-differnce PDE evaluated with solution $u$ (as formulated above) and $R(x)$ is the true solution of the PDE. Now, we need the Jacobian of system from the finite differencing of the PDE. This can be derivied either analytically or by using finite differencing once more. We include both codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_jacobian(F, u):\n",
    "    \"\"\" Exact Jacobian of finite difference model PDE from above \"\"\"\n",
    "    n = len(u)\n",
    "    h = 1 / (n + 1)\n",
    "    # TODO: allow @b,@c to be parameters\n",
    "    b = c = 1\n",
    "    expu = np.exp(u)\n",
    "\n",
    "    subdiag = -(1 / h**2) - 2 * b / (2 * h) * expu[:n - 1]\n",
    "    maindiag = 2 / (h**2) + c * expu\n",
    "    supdiag = -(1 / h**2) + 2 * b / (2 * h) * expu[1:]\n",
    "\n",
    "    J = sp.diags(subdiag, -1) + sp.diags(maindiag, 0) + sp.diags(supdiag, 1)\n",
    "\n",
    "    return J\n",
    "\n",
    "def fd_jacobian(F, u):\n",
    "    \"\"\" Finite Difference Approximation of Jacobian for any function F.\n",
    "\n",
    "    - F : Function eval. Returns 1d np.ndarray\n",
    "    - x : Current solution\n",
    "\n",
    "    Returns: @J csr_matrix\n",
    "    \"\"\"\n",
    "    n = len(u)\n",
    "    J = np.zeros((n, n))\n",
    "    slen = 1e-4\n",
    "    Fu = F(u)\n",
    "\n",
    "    for i in range(n):\n",
    "        e_i = np.append(np.zeros(i), np.append(1, np.zeros(n - i - 1)))\n",
    "        # TODO: Drop terms with small values to induce sparsity?\n",
    "        J[:, i] = (F(u + slen * e_i) - Fu) / slen\n",
    "\n",
    "    return sp.csr_matrix(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we write the code for Newton's method with a matrix of the Jacobian, we observe that in each iteration of Newton's method we need to solve the linear system $J_F(x_i)s = -F(x_i)$. We consider three methods for solving this:\n",
    "\n",
    "- Sparse direct solver\n",
    "- GMRES\n",
    "- BiCG? CGS?\n",
    "\n",
    "We recall the convergence of GMRES for solving $Ax=b$ depends on the residual bound. Assuming the matrix $A$ is diagonalizable via $A=X^{-1}\\Lambda X$ where $\\Lambda=\\mathrm{diag}(\\lambda_1,\\ldots,\\lambda_n)$, then we at the $m$ iteration that\n",
    "\n",
    "$$ \\frac{\\|r_m \\|_2}{\\|r_0\\|_2} \\leq \\kappa_2(X) \\min_{p \\in \\mathbb{P}_m, p(0)=1} \\max_{i=1,\\ldots,n} |p(\\lambda_i)|$$\n",
    "\n",
    "In other words, the non-normality of the Jacobian matrix $J$ can impact the convergence when using GMRES. Below, we examine $\\kappa(X)$ for the exact and finite-difference Jacobian matrix with different solutions for $u$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cond(X) for exact Jacobian of size n=10   with uniform solution => 9.7716e+00\n",
      "Cond(X) for exact Jacobian of size n=100  with uniform solution => 1.4370e+01\n",
      "Cond(X) for exact Jacobian of size n=1000 with uniform solution => 1.5072e+01\n",
      "===\n",
      "Cond(X) for FD Jacobian of size n=10   with uniform solution => 9.7727e+00\n",
      "Cond(X) for FD Jacobian of size n=100  with uniform solution => 1.4372e+01\n",
      "Cond(X) for FD Jacobian of size n=1000 with uniform solution => 1.5074e+01\n",
      "===\n",
      "Cond(X) for exact Jacobian of size n=10   with random solution => 3.9633e+00\n",
      "Cond(X) for exact Jacobian of size n=100  with random solution => 3.3224e+08\n",
      "Cond(X) for exact Jacobian of size n=1000 with random solution => 2.3783e+16\n",
      "===\n",
      "Cond(X) for FD Jacobian of size n=10   with random solution => 5.1872e+01\n",
      "Cond(X) for FD Jacobian of size n=100  with random solution => 2.9460e+09\n",
      "Cond(X) for FD Jacobian of size n=1000 with random solution => 2.7368e+16\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) # fix randomness \n",
    "\n",
    "# which solution @u we use\n",
    "for i in range(2):\n",
    "    # exact or FD jacobian\n",
    "    for j in range(2):\n",
    "        # size of problem\n",
    "        for k in range(1,4):\n",
    "            \n",
    "            n = 10**k\n",
    "            \n",
    "            if(i==0): u = np.ones(n)\n",
    "            else: u = 10*np.random.random(n)\n",
    "                \n",
    "            if(j==0): J = exact_jacobian(nonlinear_PDE, u)\n",
    "            else: J = fd_jacobian(nonlinear_PDE, u)\n",
    "                \n",
    "            _,X = la.eig(J.todense())\n",
    "            condX = la.cond(X)\n",
    "            \n",
    "            print(\"Cond(X) for {} Jacobian of size n={:<4} with {} solution => {:.4e}\".format(\\\n",
    "                    \"exact\" if j==0 else \"FD\",\n",
    "                    n,\n",
    "                    \"uniform\" if i==0 else \"random\",\n",
    "                    condX))\n",
    "            \n",
    "        print(\"===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see that the choice of exact or finite difference Jacobian does not make much of a difference theoretically. However, the true solution $u^\\star$ and any approximate solution $u$ encountered during the Newton's step can severely hamper the convergence of GMRES potentially. \n",
    "\n",
    "With this in mind, we move onto Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons(F, J, x0, tol=1e-6):\n",
    "    \"\"\" Newton's Method with exact or FD Jacobian\n",
    "\n",
    "    - F : Function eval. Returns 1d np.ndarray\n",
    "    - J : Jacobian eval. Returns 2d np.ndarray\n",
    "    - x0 : Initial guess. 1D np.ndarray\n",
    "\n",
    "    Returns:\n",
    "    - @x solution so that F(x)=0\n",
    "    - @numiters, int\n",
    "    - @score_hist, history of \"F(x)\". 1d np.ndarray\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    niters = 0\n",
    "    score_hist = np.array([la.norm(F(x))])\n",
    "\n",
    "    while (la.norm(F(x)) > tol):\n",
    "        A = J(F, x)\n",
    "        # use solver with/without preconditioner\n",
    "        # TODO: When will this be singular?\n",
    "        # TODO: Allow other methods like GMRES and maybe CGS\n",
    "        s = spla.spsolve(A, -F(x))\n",
    "        x = x + s\n",
    "        niters += 1\n",
    "        score_hist = np.append(score_hist, la.norm(F(x)))\n",
    "\n",
    "    return x, niters, score_hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
